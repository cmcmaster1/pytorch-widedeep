{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Binary Classification with defaults\n",
    "\n",
    "In this notebook we will use the Adult Census dataset. Download the data from [here](https://www.kaggle.com/wenruliu/adult-income-dataset/downloads/adult.csv/2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d:\\projects\\Chris Projects\\PODIUM\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.training import Trainer\n",
    "from pytorch_widedeep.models import Wide, TabMlp, TabResnet, TabTransformer, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy, Precision\n",
    "\n",
    "from pytorch_widedeep.models.tab_transformer import IntersampleAttention, MultiHeadedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code in this module is inspired by a number of implementations:\n",
    "\n",
    "Classes PositionwiseFF and AddNorm are 'stolen' with much gratitude from the fantastic d2l.ai book:\n",
    "https://d2l.ai/chapter_attention-mechanisms/transformer.html\n",
    "\n",
    "MultiHeadedAttention is inspired by the TabTransformer implementation here:\n",
    "https://github.com/lucidrains/tab-transformer-pytorch. General comment: just go and have a look to\n",
    "https://github.com/lucidrains\n",
    "\n",
    "The fixed attention implementation and SharedEmbeddings are inspired by the\n",
    "TabTransformer available in AutoGluon:\n",
    "https://github.com/awslabs/autogluon/tree/master/tabular/src/autogluon/tabular/models/tab_transformer\n",
    "If you have not checked that library, you should.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import einops\n",
    "from torch import nn, einsum\n",
    "\n",
    "from pytorch_widedeep.wdtypes import *  # noqa: F403\n",
    "from pytorch_widedeep.models.tab_mlp import MLP, _get_activation_fn\n",
    "\n",
    "class ContinuousEmbedding(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      embedding_size: int\n",
    "  ):\n",
    "      super(ContinuousEmbedding, self).__init__()\n",
    "      self.relu = nn.ReLU()\n",
    "      self.embedding = nn.Linear(1, embedding_size)\n",
    "\n",
    "  def forward(self, X: Tensor) -> Tensor:\n",
    "      return self.relu(self.embedding(X.t().unsqueeze(2)).permute(1, 0, 2))\n",
    "\n",
    "\n",
    "class PositionwiseFF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        ff_hidden_dim: int,\n",
    "        dropout: float,\n",
    "        activation: str,\n",
    "    ):\n",
    "        super(PositionwiseFF, self).__init__()\n",
    "        self.w_1 = nn.Linear(input_dim, ff_hidden_dim)\n",
    "        self.w_2 = nn.Linear(ff_hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(X))))\n",
    "\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, input_dim: int, dropout: float):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, X: Tensor, Y: Tensor) -> Tensor:\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_heads: int,\n",
    "        keep_attn_weights: bool,\n",
    "        dropout: float,\n",
    "        fixed_attention: bool,\n",
    "        num_cat_columns: int,\n",
    "    ):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "\n",
    "        assert (\n",
    "            input_dim % num_heads == 0\n",
    "        ), \"'input_dim' must be divisible by 'num_heads'\"\n",
    "        if fixed_attention and not num_cat_columns:\n",
    "            raise ValueError(\n",
    "                \"if 'fixed_attention' is 'True' the number of categorical \"\n",
    "                \"columns 'num_cat_columns' must be specified\"\n",
    "            )\n",
    "        # Consistent with other implementations I assume d_v = d_k\n",
    "        self.d_k = input_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fixed_attention = fixed_attention\n",
    "        if fixed_attention:\n",
    "            self.inp_proj = nn.Linear(input_dim, input_dim)\n",
    "            self.fixed_key = nn.init.xavier_normal_(\n",
    "                nn.Parameter(torch.empty(num_cat_columns, input_dim))\n",
    "            )\n",
    "            self.fixed_query = nn.init.xavier_normal_(\n",
    "                nn.Parameter(torch.empty(num_cat_columns, input_dim))\n",
    "            )\n",
    "        else:\n",
    "            self.inp_proj = nn.Linear(input_dim, input_dim * 3)\n",
    "        self.out_proj = nn.Linear(input_dim, input_dim)\n",
    "        self.keep_attn_weights = keep_attn_weights\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        # b: batch size, s: src seq length (num of categorical features\n",
    "        # encoded as embeddings), l: target sequence (l = s), e: embeddings\n",
    "        # dimensions, h: number of attention heads, d: d_k\n",
    "        if self.fixed_attention:\n",
    "            v = self.inp_proj(X)\n",
    "            k = einops.repeat(\n",
    "                self.fixed_key.unsqueeze(0), \"b s e -> (b copy) s e\", copy=X.shape[0]\n",
    "            )\n",
    "            q = einops.repeat(\n",
    "                self.fixed_query.unsqueeze(0), \"b s e -> (b copy) s e\", copy=X.shape[0]\n",
    "            )\n",
    "        else:\n",
    "            q, k, v = self.inp_proj(X).chunk(3, dim=2)\n",
    "        q, k, v = map(\n",
    "            lambda t: einops.rearrange(t, \"b s (h d) -> b h s d\", h=self.num_heads),\n",
    "            (q, k, v),\n",
    "        )\n",
    "        scores = einsum(\"b h s d, b h l d -> b h s l\", q, k) / math.sqrt(self.d_k)\n",
    "        attn_weights = self.dropout(scores.softmax(dim=-1))\n",
    "        if self.keep_attn_weights:\n",
    "            self.attn_weights = attn_weights\n",
    "        attn_output = einsum(\"b h s l, b h l d -> b h s d\", attn_weights, v)\n",
    "        output = einops.rearrange(attn_output, \"b h s d -> b s (h d)\", h=self.num_heads)\n",
    "\n",
    "        return self.out_proj(output)\n",
    "\n",
    "class IntersampleAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_heads: int,\n",
    "        keep_attn_weights: bool,\n",
    "        dropout: float,\n",
    "        fixed_attention: bool,\n",
    "        num_cat_columns: int,\n",
    "        num_input_columns: int\n",
    "    ):\n",
    "        super(IntersampleAttention, self).__init__()\n",
    "\n",
    "        assert (\n",
    "            input_dim % num_heads == 0\n",
    "        ), \"'input_dim' must be divisible by 'num_heads'\"\n",
    "        if fixed_attention and not num_cat_columns:\n",
    "            raise ValueError(\n",
    "                \"if 'fixed_attention' is 'True' the number of categorical \"\n",
    "                \"columns 'num_cat_columns' must be specified\"\n",
    "            )\n",
    "        # Consistent with other implementations I assume d_v = d_k\n",
    "        self.input_dim = input_dim\n",
    "        self.d_k = input_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fixed_attention = fixed_attention\n",
    "        if fixed_attention:\n",
    "            self.inp_proj = nn.Linear(input_dim, input_dim)\n",
    "            self.fixed_key = nn.init.xavier_normal_(\n",
    "                nn.Parameter(torch.empty(num_cat_columns, input_dim))\n",
    "            )\n",
    "            self.fixed_query = nn.init.xavier_normal_(\n",
    "                nn.Parameter(torch.empty(num_cat_columns, input_dim))\n",
    "            )\n",
    "        else:\n",
    "            self.inp_proj = nn.Linear(input_dim * num_input_columns, input_dim * num_input_columns * 3)\n",
    "        self.out_proj = nn.Linear(input_dim, input_dim)\n",
    "        self.keep_attn_weights = keep_attn_weights\n",
    "\n",
    "    def forward(self, X):\n",
    "        # b: batch size, s: src seq length (num of categorical features\n",
    "        # encoded as embeddings), l: target sequence (l = s), e: embeddings\n",
    "        # dimensions, h: number of attention heads, d: d_k\n",
    "        # df: embedding dim * num of features\n",
    "        X = einops.rearrange(X, \"b s e -> () b (s e)\")\n",
    "        if self.fixed_attention:\n",
    "            v = self.inp_proj(X)\n",
    "            k = einops.repeat(\n",
    "                self.fixed_key.unsqueeze(0), \"b s e -> (b copy) s e\", copy=X.shape[0]\n",
    "            )\n",
    "            q = einops.repeat(\n",
    "                self.fixed_query.unsqueeze(0), \"b s e -> (b copy) s e\", copy=X.shape[0]\n",
    "            )\n",
    "        else:\n",
    "            q, k, v = self.inp_proj(X).chunk(3, dim=2)\n",
    "            \n",
    "        q, k, v = map(\n",
    "            lambda t: einops.rearrange(t, \"() b (h df) -> () h b df\", h=self.num_heads),\n",
    "            (q, k, v),\n",
    "        )\n",
    "        scores = einsum(\"b h s d, b h l d -> b h s l\", q, k) / math.sqrt(self.d_k)\n",
    "        attn_weights = self.dropout(scores.softmax(dim=-1))\n",
    "        if self.keep_attn_weights:\n",
    "            self.attn_weights = attn_weights\n",
    "        attn_output = einsum(\"b h s l, b h l d -> b h s d\", attn_weights, v)\n",
    "        output = einops.rearrange(attn_output, \"() h b df -> () b (h df)\", h=self.num_heads)\n",
    "        output = einops.rearrange(output, \"() b (s e) -> b s e\", e = self.input_dim)\n",
    "\n",
    "        return self.out_proj(output)\n",
    "        \n",
    "class SAINTTransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_heads: int,\n",
    "        keep_attn_weights: bool,\n",
    "        ff_hidden_dim: int,\n",
    "        dropout: float,\n",
    "        activation: str,\n",
    "        fixed_attention,\n",
    "        num_cat_columns,\n",
    "        num_input_columns,\n",
    "    ):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.self_attn = MultiHeadedAttention(\n",
    "            input_dim,\n",
    "            num_heads,\n",
    "            keep_attn_weights,\n",
    "            dropout,\n",
    "            fixed_attention,\n",
    "            num_cat_columns,\n",
    "        )\n",
    "        self.is_attn = IntersampleAttention(\n",
    "            input_dim,\n",
    "            num_heads,\n",
    "            keep_attn_weights,\n",
    "            dropout,\n",
    "            fixed_attention,\n",
    "            num_cat_columns,\n",
    "            num_input_columns,\n",
    "        )\n",
    "        self.feed_forward = PositionwiseFF(\n",
    "            input_dim, ff_hidden_dim, dropout, activation\n",
    "        )\n",
    "        self.attn_addnorm = AddNorm(input_dim, dropout)\n",
    "        self.is_attn_addnorm = AddNorm(input_dim, dropout)\n",
    "        self.ff_addnorm1 = AddNorm(input_dim, dropout)\n",
    "        self.ff_addnorm2 = AddNorm(input_dim, dropout)\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        Y_self_attn = self.self_attn(X)\n",
    "        Y = self.attn_addnorm(X, Y_self_attn)\n",
    "        Y = self.ff_addnorm1(Y, self.feed_forward(Y))\n",
    "        Y_is_attn = self.is_attn(Y)\n",
    "        Y = self.is_attn_addnorm(Y, Y_is_attn)\n",
    "        return self.ff_addnorm2(Y, self.feed_forward(Y))\n",
    "\n",
    "\n",
    "class FullEmbeddingDropout(nn.Module):\n",
    "    def __init__(self, dropout: float):\n",
    "        super(FullEmbeddingDropout, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        mask = X.new().resize_((X.size(1), 1)).bernoulli_(1 - self.dropout).expand_as(\n",
    "            X\n",
    "        ) / (1 - self.dropout)\n",
    "        return mask * X\n",
    "\n",
    "\n",
    "class SharedEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embed: int,\n",
    "        embed_dim: int,\n",
    "        embed_dropout: float,\n",
    "        full_embed_dropout: bool = False,\n",
    "        add_shared_embed: bool = False,\n",
    "        frac_shared_embed=8,\n",
    "    ):\n",
    "        super(SharedEmbeddings, self).__init__()\n",
    "        assert (\n",
    "            embed_dim % frac_shared_embed == 0\n",
    "        ), \"'embed_dim' must be divisible by 'frac_shared_embed'\"\n",
    "\n",
    "        self.add_shared_embed = add_shared_embed\n",
    "        self.embed = nn.Embedding(num_embed, embed_dim, padding_idx=0)\n",
    "        self.embed.weight.data.clamp_(-2, 2)\n",
    "        if add_shared_embed:\n",
    "            col_embed_dim = embed_dim\n",
    "        else:\n",
    "            col_embed_dim = embed_dim // frac_shared_embed\n",
    "        self.shared_embed = nn.Parameter(torch.empty(1, col_embed_dim).uniform_(-1, 1))\n",
    "\n",
    "        if full_embed_dropout:\n",
    "            self.dropout = FullEmbeddingDropout(embed_dropout)\n",
    "        else:\n",
    "            self.dropout = nn.Dropout(embed_dropout)  # type: ignore[assignment]\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        out = self.dropout(self.embed(X))\n",
    "        shared_embed = self.shared_embed.expand(out.shape[0], -1)\n",
    "        if self.add_shared_embed:\n",
    "            out += shared_embed\n",
    "        else:\n",
    "            out[:, : shared_embed.shape[1]] = shared_embed\n",
    "        return out\n",
    "\n",
    "\n",
    "class SAINT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        column_idx: Dict[str, int],\n",
    "        embed_input: List[Tuple[str, int]],\n",
    "        continuous_cols: Optional[List[str]] = None,\n",
    "        embed_dropout: float = 0.1,\n",
    "        full_embed_dropout: bool = False,\n",
    "        shared_embed: bool = False,\n",
    "        add_shared_embed: bool = False,\n",
    "        frac_shared_embed: int = 8,\n",
    "        input_dim: int = 32,\n",
    "        num_heads: int = 8,\n",
    "        num_blocks: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "        keep_attn_weights: bool = False,\n",
    "        fixed_attention: bool = False,\n",
    "        num_cat_columns: Optional[int] = None,\n",
    "        ff_hidden_dim: int = 32 * 4,\n",
    "        transformer_activation: str = \"gelu\",\n",
    "        mlp_hidden_dims: Optional[List[int]] = None,\n",
    "        mlp_activation: str = \"relu\",\n",
    "        mlp_batchnorm: bool = False,\n",
    "        mlp_batchnorm_last: bool = False,\n",
    "        mlp_linear_first: bool = True,\n",
    "        embed_continuous: bool = True,\n",
    "    ):\n",
    "\n",
    "        r\"\"\"TabTransformer model (https://arxiv.org/pdf/2012.06678.pdf) model that\n",
    "        can be used as the deeptabular component of a Wide & Deep model.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        column_idx: Dict\n",
    "            Dict containing the index of the columns that will be passed through\n",
    "            the DeepDense model. Required to slice the tensors. e.g. {'education':\n",
    "            0, 'relationship': 1, 'workclass': 2, ...}\n",
    "        embed_input: List\n",
    "            List of Tuples with the column name and number of unique values\n",
    "            e.g. [(education, 11, 32), ...]\n",
    "        continuous_cols: List, Optional, default = None\n",
    "            List with the name of the numeric (aka continuous) columns\n",
    "        embed_dropout: float, default = 0.1\n",
    "            Dropout to be applied to the embeddings matrix\n",
    "        full_embed_dropout: bool, default = False\n",
    "            Boolean indicating if an entire embedding (i.e. the representation\n",
    "            for one categorical column) will be dropped in the batch. See:\n",
    "            ``pytorch_widedeep.model.tab_transformer.FullEmbeddingDropout``.\n",
    "            If ``full_embed_dropout = True``, ``embed_dropout`` is ignored.\n",
    "        shared_embed: bool, default = False\n",
    "            The idea behind ``shared_embed`` is described in the Appendix A in the paper:\n",
    "            `'The goal of having column embedding is to enable the model to distinguish the\n",
    "            classes in one column from those in the other columns'`. In other words, the idea\n",
    "            is to let the model learn which column is embedding at the time.\n",
    "        add_shared_embed: bool, default = False,\n",
    "            The two embedding sharing strategies are: 1) add the shared embeddings to the column\n",
    "            embeddings or 2) to replace the first ``frac_shared_embed`` with the shared\n",
    "            embeddings. See ``pytorch_widedeep.models.tab_transformer.SharedEmbeddings``\n",
    "        frac_shared_embed: int, default = 8\n",
    "            The fraction of embeddings that will be shared by all the different categories for\n",
    "            one particular column.\n",
    "        input_dim: int, default = 32\n",
    "            The so-called *dimension of the model*. Is the number of embeddings used to encode\n",
    "            the categorical columns\n",
    "        num_heads: int, default = 8\n",
    "            Number of attention heads per Transformer block\n",
    "        num_blocks: int, default = 6\n",
    "            Number of Transformer blocks\n",
    "        dropout: float, default = 0.1\n",
    "            Dropout that will be applied internally to the\n",
    "            ``TransformerEncoder`` (see\n",
    "            ``pytorch_widedeep.model.tab_transformer.TransformerEncoder``) and the\n",
    "            output MLP\n",
    "        keep_attn_weights: bool, default = False\n",
    "            If set to ``True`` the model will store the attention weights in the ``attention_weights``\n",
    "            attribute.\n",
    "        fixed_attention: bool, default = False\n",
    "            If set to ``True`` all the observations in a batch will have the\n",
    "            same Key and Query. This implementation is inspired by the one\n",
    "            available at the `Autogluon tabular library\n",
    "            <https://github.com/awslabs/autogluon/blob/master/tabular/src/autogluon/tabular/models/tab_transformer/modified_transformer.py>`_.\n",
    "        num_cat_columns: int, Optional, default = None\n",
    "            If `fixed_attention` is set to ``True`` the number of categorical\n",
    "            columns that will be encoded as embeddings must be specified\n",
    "        ff_hidden_dim: int, default = 128\n",
    "            Hidden dimension of the ``FeedForward`` Layer. See\n",
    "            ``pytorch_widedeep.model.tab_transformer.FeedForward``.\n",
    "        transformer_activation: str, default = \"gelu\"\n",
    "            Transformer Encoder activation function\n",
    "        mlp_hidden_dims: List, Optional, default = None\n",
    "            MLP hidden dimensions. If not provided it will default to ``[4*l,\n",
    "            2*l]`` where ``l`` is the mlp input dimension\n",
    "        mlp_activation: str, default = \"gelu\"\n",
    "            MLP activation function\n",
    "        mlp_batchnorm: bool, default = False\n",
    "            Boolean indicating whether or not to apply batch normalization to the\n",
    "            dense layers\n",
    "        mlp_batchnorm_last: bool, default = False\n",
    "            Boolean indicating whether or not to apply batch normalization to the\n",
    "            last of the dense layers\n",
    "        mlp_linear_first: bool, default = False\n",
    "            Boolean indicating whether the order of the operations in the dense\n",
    "            layer. If ``True: [LIN -> ACT -> BN -> DP]``. If ``False: [BN -> DP ->\n",
    "            LIN -> ACT]``\n",
    "        embed_continuous: bool, default = False\n",
    "            Boolean indicating whether or not continuous variables should also be\n",
    "            embedded. If true, they will pass through a variable-specific linear\n",
    "            layer with ReLU activation (see https://arxiv.org/abs/2106.01342)\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        embed_layers: ``nn.ModuleDict``\n",
    "            Dict with the embeddings per column\n",
    "        tab_transformer_blks: ``nn.Sequential``\n",
    "            Sequence of Transformer blocks\n",
    "        attention_weights: List\n",
    "            List with the attention weights per block\n",
    "        mlp: ``nn.Module``\n",
    "            MLP component in the TabTransformer model\n",
    "        output_dim: int\n",
    "            The output dimension of the model. This is a required attribute\n",
    "            neccesary to build the WideDeep class\n",
    "        continuous_embedding ``nn.Module``\n",
    "            Embeddings for the continuous variables\n",
    "\n",
    "        Example\n",
    "        --------\n",
    "        >>> import torch\n",
    "        >>> from pytorch_widedeep.models import TabTransformer\n",
    "        >>> X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)\n",
    "        >>> colnames = ['a', 'b', 'c', 'd', 'e']\n",
    "        >>> embed_input = [(u,i) for u,i in zip(colnames[:4], [4]*4)]\n",
    "        >>> continuous_cols = ['e']\n",
    "        >>> column_idx = {k:v for v,k in enumerate(colnames)}\n",
    "        >>> model = TabTransformer(column_idx=column_idx, embed_input=embed_input, continuous_cols=continuous_cols)\n",
    "        >>> out = model(X_tab)\n",
    "        \"\"\"\n",
    "        super(TabTransformer, self).__init__()\n",
    "\n",
    "        self.column_idx = column_idx\n",
    "        self.embed_input = embed_input\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.embed_dropout = embed_dropout\n",
    "        self.full_embed_dropout = full_embed_dropout\n",
    "        self.shared_embed = shared_embed\n",
    "        self.add_shared_embed = add_shared_embed\n",
    "        self.frac_shared_embed = frac_shared_embed\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.dropout = dropout\n",
    "        self.keep_attn_weights = keep_attn_weights\n",
    "        self.fixed_attention = fixed_attention\n",
    "        self.num_cat_columns = num_cat_columns\n",
    "        self.ff_hidden_dim = ff_hidden_dim\n",
    "        self.transformer_activation = transformer_activation\n",
    "        self.mlp_hidden_dims = mlp_hidden_dims\n",
    "        self.mlp_activation = mlp_activation\n",
    "        self.mlp_batchnorm = mlp_batchnorm\n",
    "        self.mlp_batchnorm_last = mlp_batchnorm_last\n",
    "        self.mlp_linear_first = mlp_linear_first\n",
    "        self.embed_continuous = embed_continuous\n",
    "\n",
    "        # Embeddings: val + 1 because 0 is reserved for padding/unseen cateogories.\n",
    "        if shared_embed:\n",
    "            self.embed_layers = nn.ModuleDict(\n",
    "                {\n",
    "                    \"emb_layer_\"\n",
    "                    + col: SharedEmbeddings(\n",
    "                        val + 1,\n",
    "                        input_dim,\n",
    "                        embed_dropout,\n",
    "                        full_embed_dropout,\n",
    "                        add_shared_embed,\n",
    "                        frac_shared_embed,\n",
    "                    )\n",
    "                    for col, val in self.embed_input\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            self.embed_layers = nn.ModuleDict(\n",
    "                {\n",
    "                    \"emb_layer_\" + col: nn.Embedding(val + 1, input_dim, padding_idx=0)\n",
    "                    for col, val in self.embed_input\n",
    "                }\n",
    "            )\n",
    "            if full_embed_dropout:\n",
    "                self.embedding_dropout = FullEmbeddingDropout(embed_dropout)\n",
    "            else:\n",
    "                self.embedding_dropout = nn.Dropout(embed_dropout)  # type: ignore[assignment]\n",
    "        # Continuous\n",
    "        if self.continuous_cols is not None:\n",
    "            cont_inp_dim = len(self.continuous_cols)\n",
    "        else:\n",
    "            cont_inp_dim = 0\n",
    "\n",
    "        if self.embed_continuous:\n",
    "            self.continuous_embedding = ContinuousEmbedding(self.input_dim)\n",
    "\n",
    "        if embed_continuous:\n",
    "            num_input_columns = len(embed_input) + cont_inp_dim\n",
    "        else:\n",
    "            num_input_columns = len(embed_input)\n",
    "\n",
    "        self.tab_transformer_blks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.tab_transformer_blks.add_module(\n",
    "                \"block\" + str(i),\n",
    "                SAINTTransformerEncoder(\n",
    "                    input_dim,\n",
    "                    num_heads,\n",
    "                    keep_attn_weights,\n",
    "                    ff_hidden_dim,\n",
    "                    dropout,\n",
    "                    transformer_activation,\n",
    "                    fixed_attention,\n",
    "                    num_cat_columns,\n",
    "                    num_input_columns,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        if keep_attn_weights:\n",
    "            self.attention_weights = [None] * num_blocks\n",
    "\n",
    "        if not mlp_hidden_dims:\n",
    "            if embed_continuous:\n",
    "                mlp_inp_l = (len(embed_input) + cont_inp_dim) * input_dim\n",
    "            else:\n",
    "                mlp_inp_l = (len(embed_input) * input_dim) + cont_inp_dim\n",
    "            mlp_hidden_dims = [mlp_inp_l, mlp_inp_l * 4, mlp_inp_l * 2]\n",
    "\n",
    "        self.tab_transformer_mlp = MLP(\n",
    "            mlp_hidden_dims,\n",
    "            mlp_activation,\n",
    "            dropout,\n",
    "            mlp_batchnorm,\n",
    "            mlp_batchnorm_last,\n",
    "            mlp_linear_first,\n",
    "        )\n",
    "\n",
    "        # the output_dim attribute will be used as input_dim when \"merging\" the models\n",
    "        self.output_dim = mlp_hidden_dims[-1]\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "\n",
    "        embed = [\n",
    "            self.embed_layers[\"emb_layer_\" + col](\n",
    "                X[:, self.column_idx[col]].long()\n",
    "            ).unsqueeze(1)\n",
    "            for col, _ in self.embed_input\n",
    "        ]\n",
    "        x = torch.cat(embed, 1)\n",
    "        if not self.shared_embed and self.embedding_dropout is not None:\n",
    "            x = self.embedding_dropout(x)\n",
    "\n",
    "        if self.continuous_cols is not None and self.embed_continuous:\n",
    "            cont_idx = [self.column_idx[col] for col in self.continuous_cols]\n",
    "            x_cont = X[:, cont_idx].float()\n",
    "            x_cont = self.continuous_embedding(x_cont)\n",
    "            x_cont = self.embedding_dropout(x_cont)\n",
    "            x = torch.cat([x, x_cont], 1)\n",
    "\n",
    "        for i, blk in enumerate(self.tab_transformer_blks):\n",
    "            x = blk(x)\n",
    "            if self.keep_attn_weights:\n",
    "                self.attention_weights[i] = blk.self_attn.attn_weights\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        if self.continuous_cols is not None and self.embed_continuous==False:\n",
    "            cont_idx = [self.column_idx[col] for col in self.continuous_cols]\n",
    "            x_cont = X[:, cont_idx].float()\n",
    "            x = torch.cat([x, x_cont], 1)\n",
    "\n",
    "        return self.tab_transformer_mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([91, 6, 32])"
      ]
     },
     "metadata": {},
     "execution_count": 277
    }
   ],
   "source": [
    "is_attn = IntersampleAttention(\n",
    "        input_dim = 32,\n",
    "        num_heads = 8,\n",
    "        keep_attn_weights = False,\n",
    "        dropout = 0.1,\n",
    "        fixed_attention = False,\n",
    "        num_cat_columns = 3,\n",
    "        num_input_columns = 6)\n",
    "\n",
    "torch_test = torch.rand([91, 6, 32])\n",
    "\n",
    "out = is_attn(torch_test)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d:\\projects\\Chris Projects\\PODIUM\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>educational-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>gender</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n      <th>income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25</td>\n      <td>Private</td>\n      <td>226802</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Never-married</td>\n      <td>Machine-op-inspct</td>\n      <td>Own-child</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>89814</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Married-civ-spouse</td>\n      <td>Farming-fishing</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28</td>\n      <td>Local-gov</td>\n      <td>336951</td>\n      <td>Assoc-acdm</td>\n      <td>12</td>\n      <td>Married-civ-spouse</td>\n      <td>Protective-serv</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44</td>\n      <td>Private</td>\n      <td>160323</td>\n      <td>Some-college</td>\n      <td>10</td>\n      <td>Married-civ-spouse</td>\n      <td>Machine-op-inspct</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>7688</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18</td>\n      <td>?</td>\n      <td>103497</td>\n      <td>Some-college</td>\n      <td>10</td>\n      <td>Never-married</td>\n      <td>?</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 278
    }
   ],
   "source": [
    "df = pd.read_csv('data/adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d:\\projects\\Chris Projects\\PODIUM\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational_num      marital_status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital_gain  capital_loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours_per_week native_country  income_label  \n",
       "0              40  United-States             0  \n",
       "1              50  United-States             0  \n",
       "2              40  United-States             1  \n",
       "3              40  United-States             1  \n",
       "4              30  United-States             0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>educational_num</th>\n      <th>marital_status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>gender</th>\n      <th>capital_gain</th>\n      <th>capital_loss</th>\n      <th>hours_per_week</th>\n      <th>native_country</th>\n      <th>income_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25</td>\n      <td>Private</td>\n      <td>226802</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Never-married</td>\n      <td>Machine-op-inspct</td>\n      <td>Own-child</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>89814</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Married-civ-spouse</td>\n      <td>Farming-fishing</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n      <td>United-States</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28</td>\n      <td>Local-gov</td>\n      <td>336951</td>\n      <td>Assoc-acdm</td>\n      <td>12</td>\n      <td>Married-civ-spouse</td>\n      <td>Protective-serv</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44</td>\n      <td>Private</td>\n      <td>160323</td>\n      <td>Some-college</td>\n      <td>10</td>\n      <td>Married-civ-spouse</td>\n      <td>Machine-op-inspct</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>7688</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18</td>\n      <td>?</td>\n      <td>103497</td>\n      <td>Some-college</td>\n      <td>10</td>\n      <td>Never-married</td>\n      <td>?</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>United-States</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 279
    }
   ],
   "source": [
    "# For convenience, we'll replace '-' with '_'\n",
    "df.columns = [c.replace(\"-\", \"_\") for c in df.columns]\n",
    "# binary target\n",
    "df['income_label'] = (df[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "df.drop('income', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Have a look to notebooks one and two if you want to get a good understanding of the next few lines of code (although there is no need to use the package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d:\\projects\\Chris Projects\\PODIUM\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "wide_cols = ['education', 'relationship','workclass','occupation','native_country','gender']\n",
    "crossed_cols = [('education', 'occupation'), ('native_country', 'occupation')]\n",
    "cat_embed_cols = [('education',16), ('relationship',8), ('workclass',16), ('occupation',16),('native_country',16)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "target_col = 'income_label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET\n",
    "target = df[target_col].values\n",
    "\n",
    "# wide\n",
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "X_wide = wide_preprocessor.fit_transform(df)\n",
    "\n",
    "# deeptabular\n",
    "tab_preprocessor = TabPreprocessor(embed_cols=cat_embed_cols, continuous_cols=continuous_cols)\n",
    "X_tab = tab_preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  1  17  23 ...  89  91 316]\n",
      " [  2  18  23 ...  89  92 317]\n",
      " [  3  18  24 ...  89  93 318]\n",
      " ...\n",
      " [  2  20  23 ...  90 103 323]\n",
      " [  2  17  23 ...  89 103 323]\n",
      " [  2  21  29 ...  90 115 324]]\n",
      "(48842, 8)\n",
      "d:\\projects\\Chris Projects\\PODIUM\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(X_wide)\n",
    "print(X_wide.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.          1.          1.         ...  1.         -0.99512893\n  -0.03408696]\n [ 2.          2.          1.         ...  1.         -0.04694151\n   0.77292975]\n [ 3.          2.          2.         ...  1.         -0.77631645\n  -0.03408696]\n ...\n [ 2.          4.          1.         ...  1.          1.41180837\n  -0.03408696]\n [ 2.          1.          1.         ...  1.         -1.21394141\n  -1.64812038]\n [ 2.          5.          7.         ...  1.          0.97418341\n  -0.03408696]]\n(48842, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_tab)\n",
    "print(X_tab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `TabTransformer` as the `deeptabular` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for TabTransformer we only need the names of the columns\n",
    "cat_embed_cols_for_transformer = [el[0] for el in cat_embed_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['education', 'relationship', 'workclass', 'occupation', 'native_country']"
      ]
     },
     "metadata": {},
     "execution_count": 285
    }
   ],
   "source": [
    "cat_embed_cols_for_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeptabular\n",
    "tab_preprocessor = TabPreprocessor(embed_cols=cat_embed_cols_for_transformer, \n",
    "                                   continuous_cols=continuous_cols, \n",
    "                                   for_tabtransformer=True)\n",
    "X_tab = tab_preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d:\\projects\\Chris Projects\\PODIUM\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "deeptabular = TabTransformer(column_idx=tab_preprocessor.column_idx,\n",
    "                             embed_input=tab_preprocessor.embeddings_input,\n",
    "                             continuous_cols=continuous_cols,\n",
    "                             embed_continuous=True)\n",
    "model = WideDeep(wide=wide, deeptabular=deeptabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, objective='binary', metrics=[Accuracy, Precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1: 100%|██████████| 306/306 [01:14<00:00,  4.13it/s, loss=0.401, metrics={'acc': 0.8103, 'prec': 0.6269}]\n",
      "valid: 100%|██████████| 77/77 [00:53<00:00,  1.45it/s, loss=0.374, metrics={'acc': 0.8267, 'prec': 0.6523}]\n",
      "epoch 2: 100%|██████████| 306/306 [01:09<00:00,  4.38it/s, loss=0.371, metrics={'acc': 0.8244, 'prec': 0.6616}]\n",
      "valid: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s, loss=0.364, metrics={'acc': 0.8308, 'prec': 0.6732}]\n",
      "epoch 3: 100%|██████████| 306/306 [01:12<00:00,  4.22it/s, loss=0.364, metrics={'acc': 0.8281, 'prec': 0.6706}]\n",
      "valid: 100%|██████████| 77/77 [00:52<00:00,  1.45it/s, loss=0.362, metrics={'acc': 0.8349, 'prec': 0.6662}]\n",
      "epoch 4: 100%|██████████| 306/306 [01:12<00:00,  4.21it/s, loss=0.356, metrics={'acc': 0.8313, 'prec': 0.6765}]\n",
      "valid: 100%|██████████| 77/77 [00:52<00:00,  1.46it/s, loss=0.355, metrics={'acc': 0.838, 'prec': 0.6902}]\n",
      "epoch 5: 100%|██████████| 306/306 [01:12<00:00,  4.23it/s, loss=0.353, metrics={'acc': 0.8355, 'prec': 0.6873}]\n",
      "valid: 100%|██████████| 77/77 [00:50<00:00,  1.51it/s, loss=0.351, metrics={'acc': 0.8397, 'prec': 0.6992}]\n",
      "epoch 6: 100%|██████████| 306/306 [01:15<00:00,  4.03it/s, loss=0.351, metrics={'acc': 0.8353, 'prec': 0.6852}]\n",
      "valid: 100%|██████████| 77/77 [00:53<00:00,  1.45it/s, loss=0.354, metrics={'acc': 0.8355, 'prec': 0.677}]\n",
      "epoch 7: 100%|██████████| 306/306 [01:14<00:00,  4.13it/s, loss=0.349, metrics={'acc': 0.8367, 'prec': 0.6899}]\n",
      "valid: 100%|██████████| 77/77 [00:52<00:00,  1.46it/s, loss=0.354, metrics={'acc': 0.837, 'prec': 0.6907}]\n",
      "epoch 8: 100%|██████████| 306/306 [01:12<00:00,  4.21it/s, loss=0.352, metrics={'acc': 0.8346, 'prec': 0.6862}]\n",
      "valid: 100%|██████████| 77/77 [00:52<00:00,  1.47it/s, loss=0.354, metrics={'acc': 0.8366, 'prec': 0.6837}]\n",
      "epoch 9: 100%|██████████| 306/306 [01:13<00:00,  4.19it/s, loss=0.347, metrics={'acc': 0.8366, 'prec': 0.6893}]\n",
      "valid: 100%|██████████| 77/77 [00:52<00:00,  1.45it/s, loss=0.351, metrics={'acc': 0.8362, 'prec': 0.6742}]\n",
      "epoch 10: 100%|██████████| 306/306 [01:14<00:00,  4.13it/s, loss=0.346, metrics={'acc': 0.8385, 'prec': 0.6958}]\n",
      "valid: 100%|██████████| 77/77 [00:53<00:00,  1.45it/s, loss=0.349, metrics={'acc': 0.839, 'prec': 0.6959}]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(X_wide=X_wide, X_tab=X_tab, target=target, n_epochs=10, batch_size=128, val_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also mentioning that one could build a model with the individual components independently. For example, a model comprised only by the `wide` component would be simply a linear model. This could be attained by just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "deeptabular = TabTransformer(column_idx=tab_preprocessor.column_idx,\n",
    "                             embed_input=tab_preprocessor.embeddings_input,\n",
    "                             continuous_cols=continuous_cols,\n",
    "                             embed_continuous = False)\n",
    "model = WideDeep(wide=wide, deeptabular=deeptabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, objective='binary', metrics=[Accuracy, Precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1: 100%|██████████| 611/611 [00:15<00:00, 40.18it/s, loss=0.385, metrics={'acc': 0.819, 'prec': 0.6495}]\n",
      "valid: 100%|██████████| 153/153 [00:01<00:00, 97.30it/s, loss=0.372, metrics={'acc': 0.8322, 'prec': 0.6831}]\n",
      "epoch 2: 100%|██████████| 611/611 [00:15<00:00, 40.39it/s, loss=0.364, metrics={'acc': 0.8282, 'prec': 0.6697}]\n",
      "valid: 100%|██████████| 153/153 [00:01<00:00, 99.11it/s, loss=0.363, metrics={'acc': 0.8312, 'prec': 0.6605}]\n",
      "epoch 3: 100%|██████████| 611/611 [00:14<00:00, 40.75it/s, loss=0.357, metrics={'acc': 0.8323, 'prec': 0.6784}]\n",
      "valid: 100%|██████████| 153/153 [00:01<00:00, 98.44it/s, loss=0.361, metrics={'acc': 0.8319, 'prec': 0.6642}]\n",
      "epoch 4: 100%|██████████| 611/611 [00:14<00:00, 40.97it/s, loss=0.353, metrics={'acc': 0.8346, 'prec': 0.6838}]\n",
      "valid: 100%|██████████| 153/153 [00:01<00:00, 93.52it/s, loss=0.356, metrics={'acc': 0.8342, 'prec': 0.6679}]\n",
      "epoch 5: 100%|██████████| 611/611 [00:15<00:00, 40.24it/s, loss=0.348, metrics={'acc': 0.8371, 'prec': 0.6894}]\n",
      "valid: 100%|██████████| 153/153 [00:01<00:00, 98.96it/s, loss=0.353, metrics={'acc': 0.8363, 'prec': 0.6742}]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(X_wide=X_wide, X_tab=X_tab, target=target, n_epochs=5, batch_size=64, val_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideDeep(wide=wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, objective='binary', metrics=[Accuracy, Precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 1:   0%|          | 0/611 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'deeptabular'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f54296245fed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_wide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_wide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/utils/general_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 ] = alias\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/training/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_wide, X_tab, X_text, X_img, X_train, X_val, val_split, target, n_epochs, validation_freq, batch_size, patience, finetune, finetune_epochs, finetune_max_lr, finetune_deeptabular_gradual, finetune_deeptabular_max_lr, finetune_deeptabular_layers, finetune_deeptext_gradual, finetune_deeptext_max_lr, finetune_deeptext_layers, finetune_deepimage_gradual, finetune_deepimage_max_lr, finetune_deepimage_layers, finetune_routine, stop_after_finetuning)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargett\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch %i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m                     \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargett\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                         t.set_postfix(\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/training/trainer.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, data, target, batch_idx)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/gdrive/MyDrive/pytorch-widedeep/pytorch_widedeep/models/wide_deep.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeptabular\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeptabular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"deeptabular\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeptext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeptext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"deeptext\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'deeptabular'"
     ]
    }
   ],
   "source": [
    "trainer.fit(X_wide=X_wide, target=target, n_epochs=10, batch_size=64, val_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only requisite is that the model component must be passed to `WideDeep` before \"fed\" to the `Trainer`. This is because the `Trainer` is coded so that it trains a model that has a parent called `model` and then children that correspond to the model components: `wide`,  `deeptabular`, `deeptext` and `deepimage`. Also, `WideDeep` builds the last connection between the output of those components and the final, output neuron(s)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python_cuda",
   "display_name": "Python 3.8.8 64-bit ('pytorch_cuda': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "f9de7a6cfd933f97faecc290d5f73af13d321c30cae640e0c358d00f6937371b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}